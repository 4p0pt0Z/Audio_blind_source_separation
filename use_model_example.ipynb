{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a trained model to separate audio files\n",
    "\n",
    "In order to use a trained model to run separation on an audio file, we need to:\n",
    "- load the model from checkpoint\n",
    "- be able to perform the same audio processing as the one used to transform the audio to features during training\n",
    "\n",
    "The class AudioSeparator implements the loading of model from checkpoint and the instantiation of the validation set (of the dataset that was used to train the model). The validation set implements the audio processing performed during training, so we can thus use it for our purpose.  \n",
    "*Note*: the instantiation of the AudioSeparator class will create the validation set, which can be slow if the validation set creation requires a lot of work (eg. load a lot of files into RAM). While this is acceptable for most datasets, for practical applications it should be avoided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports that we will need\n",
    "import librosa  # for audio saving\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from separator import AudioSeparator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AudioSeparator class needs 2 parameters: the checkpoint of the model to load, and the path to a folder to store the separated audio. We won't need to use the folder, so we can pass any string for this argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separated_audio_folder = \"\"  # anything will do\n",
    "# Path to the trained model checkpoint\n",
    "model_ckpt = 'path_to_mode_checkpoint.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the AudioSeparator\n",
    "separator = AudioSeparator.from_checkpoint({\"checkpoint_path\": model_ckpt, \"separated_audio_folder\": separated_audio_folder})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to load the audio that we want to perform source separation upon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load aAudioSeparatorratorSeparatorio /home/similar way as during training\n",
    "audio = separator.data_set.load_audio(\"path_to_wav_to_separate.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute audio features similarly as in training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute short-time Fourier transform\n",
    "magnitude, phase = separator.data_set.separated_stft(audio)\n",
    "\n",
    "# Go from magnitude spectrogram to actual features used during training\n",
    "features = separator.data_set.stft_magnitude_to_features(magnitude=magnitude)\n",
    "features = torch.tensor(features).unsqueeze(0)  # convert to torch tensor and add channel dimension\n",
    "# Scale the features as done during the training\n",
    "if separator.data_set.config['scaling_type'].lower() != \"none\":\n",
    "    features = separator.data_set.shift_and_scale_features(features,\n",
    "                                                           separator.data_set.config['shift'],\n",
    "                                                           separator.data_set.config['scaling'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Most) models can only process input features of a fixed shape, so the features need to be split in chunks of the right shape. The frequency shape and channel shape are decided by the processing, so we just need to split along time dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_shape = separator.data_set.features_shape()  # (channel, frequency, time)\n",
    "# Make chunks along time dimension, and stack them in a newly created batch dimension\n",
    "# Note: the last chunk which would have a smaller size than required is discarded (equivalent to truncate input audio)\n",
    "# shape of batch: [n_chunks, channel, frequency, time]\n",
    "batch = torch.stack([features[..., i*features_shape[-1]:(i+1)*features_shape[-1]] \n",
    "                     for i in range(features.shape[-1]//features_shape[-1])], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, masks = separator.model(batch)  # Labels have no utility for separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of masks: (n_chunks, n_classes, frequency, time).  \n",
    "In order to separate for a specific class: we need to know which mask to select. The classes used in training are in separator.data_set.classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(\"%s: %s\" % (class_name,idx) \n",
    "                for (idx, class_name) in {idx: class_name for idx, class_name in enumerate(separator.data_set.classes)}.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_idx = 9  # Fill here the class you are interested in !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to plot the masks and spectrograms\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# chunk_idx = 4\n",
    "\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# h0 = axs[0].imshow(masks[chunk_idx][class_idx].detach(), aspect='auto', origin='lower')\n",
    "# h1 = axs[1].imshow(batch[chunk_idx].detach().squeeze(), aspect='auto', origin='lower')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the separated spectrograms for all the sources in the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrograms = [separator.separate_spectrogram_in_lin_scale(masks[i].detach(),\n",
    "                                                            features_shape,\n",
    "                                                            magnitude[..., i*features_shape[-1]:(i+1)*features_shape[-1]])\n",
    "                for i in range(batch.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now select the class we are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the class we are interested in: \n",
    "class_spectrograms = [spec[class_idx].squeeze() for spec in spectrograms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the spectrograms together to have a single spectrogram for the entire recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate along time dimension to produce a single spectrogram for the entire recording:\n",
    "source_spectrogram = np.concatenate(class_spectrograms, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetize the separated audio from the separated spectrogram and the mixture phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to truncate the phase too.\n",
    "separated_audio = separator.spectrogram_to_audio(source_spectrogram, phase[..., :source_spectrogram.shape[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the audio to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.output.write_wav(\"path_to_output.wav\", separated_audio, sr=separator.data_set.config['sampling_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: To play audio in jupyter notebook, one can use  \n",
    "        - IPython.display.Audio  (not installed in the environement by default)  \n",
    "However, the backend playing the audio automatically normalizes the played audio (so that the max value is 1) so the amplitude of the played sound with this method is not relevant !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
