import os
import argparse
import collections
from collections import defaultdict
import json

import librosa
import numpy as np

from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit


def parse_label_file(filename, classes_list):
    """Read the events timestamps from a label file (.txt) produced by CloudFactory.

    Args:
        filename (str): path to the label file (.txt file as generated by cloud-factory when labelling the segments)
        classes_list (list): List of str: The classes susceptible to be present in the label file

    Returns:
        List of length len(classes_list). Each elements contains the starting and ending times (if any, alterning) of
        the
        labelled portion of the segment, for a class
        Eg: [[start, end, start, end], [], [], [], [start, end]]
    """

    # read label file content
    with open(filename, 'r') as file:
        lines = file.readlines()
    lines = [x.strip() for x in lines]
    # Parse the time stamps per classes if any
    timestamps = []
    for a_class in classes_list:
        # find idx of line with the class name
        class_line_idx = [idx for idx, line in enumerate(lines)
                          if line.find(a_class) > - 1
                          and line.find('.eaf') == -1
                          and line.find('AM') == -1
                          and line.find('PM') == -1]
        for line_idx_idx, line_idx in enumerate(class_line_idx):
            # time stamps should be the line after the class name
            segments_lines_idx = line_idx + 1

            if segments_lines_idx < len(lines):
                if line_idx_idx == 0:
                    timestamps.append(np.array(list(
                        map(float, filter(None,
                                          lines[segments_lines_idx].replace('TC', '')
                                          .replace('-', '\t')
                                          .replace('00:00:', '').split('\t'))))))
                else:
                    additional_line_timestamps = np.array(list(
                        map(float, filter(None,
                                          lines[segments_lines_idx].replace('TC', '')
                                          .replace('-', '\t')
                                          .replace('00:00:', '').split('\t')))))
                    timestamps[-1] = np.array(timestamps[-1].tolist() + additional_line_timestamps.tolist())
            else:
                timestamps.append(np.array([]))  # Sometimes the class name is present but there are no timestamps
        # class name is not present in the label file
        if not class_line_idx:
            timestamps.append(np.array([]))  # Sometimes there is no class name and no timestamps

    return timestamps


def segment_relative_timestamps(segment_start, segment_end, timestamps):
    """ Converts timestamps for a global recording to timestamps in a segment given the segment boundaries

    Args:
        segment_start (float): segment start time in seconds
        segment_end (float): segment end time in seconds
        timestamps (list): List with length the number of labelled classes. Each element of the list is a array of
        start and end time of labelled portion of the recording.

    Returns:
        List of the timestamps of labelled portion in the segment , with respect to the segment.

    Examples:
        >>> timestamps = [np.array([0.0, 1.0, 2.0, 9.0]),
                          np.array([0.5, 1.5]),
                          np.array([3.0, 6.0]),
                          np.array([]),
                          np.array([7.0, 8.0])]
        >>> segment_relative_timestamps(3.3, 6.6, timestamps)
        >>> [array([[0. , 3.3]], dtype=float32),
             array([], dtype=float32),
             array([[0. , 2.7]], dtype=float32),
             array([], dtype=float32),
             array([], dtype=float32)]
    """

    segment_timestamps = []
    # loop over the classes
    for c_timestamps in timestamps:
        if c_timestamps.size > 0:  # "if there are timestamps"
            inside_timestamps = []
            # For all timestamps, look if they fall in the segment. If they do, convert them to segment times.
            for (start, end) in zip(c_timestamps[::2], c_timestamps[1::2]):
                if start <= segment_end and end >= segment_start:
                    inside_timestamps.append(
                        (np.max([segment_start, start]) - segment_start, np.min([end, segment_end]) - segment_start))
            segment_timestamps.append(np.asarray(inside_timestamps, dtype=np.float32))
        else:
            segment_timestamps.append(np.array([], dtype=np.float32))
    return segment_timestamps


def label_duration_in_segments(segment_start, segment_end, label_start_times, label_end_times):
    """ Get the length (in s.) of audio which is labelled in the segment

        This function should be called with the segment starting and ending times, and the start time and end times
        of the labelled portion of a segment (for only 1 label).

    Args:
        segment_start (float): start time of audio segment in second
        segment_end (float): end time of audio segment in second
        label_start_times (np.ndarra): 1d array: contains the starting times of the labelled portions of the segment
        label_end_times (np.ndarray): 1d array: contains the ending times of the labelled portions of the segment

    Returns:
        Total duration (float) in seconds of each event class in the segment
    """

    smaller_start_times = label_start_times[label_start_times <= segment_end]
    greater_end_times = label_end_times[label_end_times >= segment_start]
    if smaller_start_times.size == 0 or greater_end_times.size == 0:
        return 0.0
    in_seg_start_times = smaller_start_times[smaller_start_times >= segment_start]
    in_seg_end_times = greater_end_times[greater_end_times <= segment_end]
    if in_seg_end_times.size == 0:
        in_seg_end_times = np.array([segment_end])
    if in_seg_start_times.size == 0:
        in_seg_start_times = np.array([segment_start])

    duration = 0.0
    if in_seg_end_times[0] <= in_seg_start_times[0]:
        duration += in_seg_end_times[0] - segment_start
        in_seg_end_times = in_seg_end_times[1:]
    if in_seg_end_times.size == 0 or in_seg_start_times[-1] >= in_seg_end_times[-1]:
        duration += segment_end - in_seg_start_times[-1]
        in_seg_start_times = in_seg_start_times[:-1]
    duration += np.sum(in_seg_end_times - in_seg_start_times)

    return duration


def generate_segments(all_wavs_filenames, all_labels_filenames, config, classes):
    r"""Produces the audio files of the segments from the audio files from Audioset and their label files,
        and returns the total filenames, labels, durations and timestamps.

        Some files in Audioset appears twice (they belong to 2 category at the same time), therefore we need to keep
        track of the produced segments to merge the duplicates, and also to perform train-test-val split,
        so we can not saved the labels and timestamps right away with the audio data.

    Args:
        all_wavs_filenames (list): List of all the wav files from Audioset
        all_labels_filenames (list): List of all the label files from CloudFactory for the files in all_wavs_filenames
        config (dict): Configuration dictionary with audio processing parameters
        classes (list): list of the classes (labels) in the segments.

    Returns:
        segment names, segment labels, segment labels durations, segment label timestamps
    """

    # Create queue to append the results
    labels = collections.deque()
    durations = collections.deque()
    segment_names = collections.deque()
    segments_timestamps = collections.deque()

    for idx, (audio_file, label_file) in enumerate(zip(all_wavs_filenames, all_labels_filenames)):
        try:  # load audio and read label. If fail: print filename and continue
            audio, _ = librosa.core.load(audio_file, sr=config["sampling_rate"], mono=True)
            labels_segment = parse_label_file(label_file, classes)
            if len(labels_segment) != len(classes):
                raise ValueError(
                    'Length of labels_segment is ' + str(len(labels_segment)) + ' while there are only ' + str(
                        len(classes)) + ' classes.')
        except Exception as e:
            print(e)
            print(audio_file)
            continue

        # Split into segments of the wanted length
        segment_n_samples = int(config['length_segments_s'] * config['sampling_rate'])
        n_seg_in_audio = audio.shape[0] // segment_n_samples
        audio = audio[:n_seg_in_audio * segment_n_samples]
        segments = np.split(audio, n_seg_in_audio)

        # Get the segments durations and timestamps and labels and save the segment audio.
        for seg_idx in range(len(segments)):
            try:
                durations.append(np.array([label_duration_in_segments(seg_idx * config['length_segments_s'],
                                                                      (seg_idx + 1) * config['length_segments_s'],
                                                                      class_label_segments[::2],
                                                                      class_label_segments[1::2])
                                           for class_label_segments in labels_segment]))
                segments_timestamps.append(segment_relative_timestamps(seg_idx * config['length_segments_s'],
                                                                       (seg_idx + 1) * config['length_segments_s'],
                                                                       labels_segment))
            except Exception as e:
                print(e, labels_segment)
            # The segment get a label for a class if 0.1
            seg_labels = (np.array(durations[-1]) >= config['label_threshold'] * config['length_segments_s'])
            labels.append(seg_labels)

            name = os.path.join(config["output_folder"],
                                os.path.splitext(os.path.basename(audio_file))[0]
                                + '_seg{}'.format(seg_idx) + '.wav')
            segment_names.append(os.path.basename(name))
            librosa.output.write_wav(name, segments[seg_idx], sr=config["sampling_rate"], norm=False)

    return np.array(segment_names), np.array(labels), np.array(durations, dtype=np.float32), \
           np.array(segments_timestamps)


def merge_remove_duplicates(segment_names, labels, durations, segments_timestamps):
    r"""Merge duplicate segments which have been labelled for 2 distinct category independently. Inplace.

    Returns:
        Merged features
    """

    def list_duplicates(seq):
        """ Returns a dict which keys are the duplicates value in the list, and the values are the duplicates
        indices. """
        tally = defaultdict(list)
        for i, item in enumerate(seq):
            tally[item].append(i)
        return ((key, locs) for key, locs in tally.items()
                if len(locs) > 1)

    duplicates = dict(list_duplicates(segment_names))

    # merge the labels, durations and timestamps
    for _, value in duplicates.items():
        for duplicate_idx in value[1:]:
            labels[value[0]] = np.maximum(labels[value[0]], labels[duplicate_idx])
            # Do not add the duration for the 'Human Speech' class as it is labelled in both files !
            durations[value[0]][:-1] += durations[duplicate_idx][:-1]
            # Instead, take the mean (equally trust labels assigned for each class)
            durations[value[0]][-1] = np.mean([durations[value[0]][-1], durations[duplicate_idx][-1]])

            # For all classes but speech, can not have timestamps for the same class in both files
            # So we take the timestamps of the duplicate if it has timestamps for a class and the initial does not.
            for c_idx in range(segments_timestamps.shape[1] - 1):
                if segments_timestamps[value[0]][c_idx].size < segments_timestamps[duplicate_idx][c_idx].size:
                    segments_timestamps[value[0]][c_idx] = segments_timestamps[duplicate_idx][c_idx]
            # For speech, take time stamps of first value. (it should be correctly labelled...)

    to_remove_indices = [index for indices in duplicates.values() for index in indices[1:]]

    return np.delete(segment_names, to_remove_indices, axis=0), \
           np.delete(labels, to_remove_indices, axis=0), \
           np.delete(durations, to_remove_indices, axis=0), \
           np.delete(segments_timestamps, to_remove_indices, axis=0)


def split_files(segment_names, labels, durations, segments_timestamps, config):
    r"""Make stratified (multilabel) training, testing and validation split of the segments, and add a prefix to the
        segment name to indicate to which set it belongs.

    Args:
        segment_names (np.ndarray): List of segment audio files.
        labels (np.ndarray): audio segments labels
        durations (np.ndarray): audio segments labelled portion durations.
        segments_timestamps (np.ndarray): Time stamps of the audio events in the segments.
        config (dict): Configuration dictionary

    Returns:
        The updated names of the audio segments.
    """

    # Stratified split
    tr_sss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=53)
    train_index, dev_test_index = next(tr_sss.split(np.zeros(labels.shape[0]), labels))
    dev_test_sss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=32)
    dev_index, test_index = next(
        dev_test_sss.split(np.zeros(labels[dev_test_index].shape[0]), labels[dev_test_index]))

    def array_to_list(array):
        r"""Recursive implementation of tolist function for numpy array"""
        if isinstance(array, np.ndarray):
            return array_to_list(array.tolist())
        elif isinstance(array, list):
            return [array_to_list(item) for item in array]
        elif isinstance(array, tuple):
            return tuple(array_to_list(item) for item in array)
        else:
            return array

    for idx, file in enumerate(segment_names):
        with open(os.path.join(config['output_folder'], os.path.splitext(file)[0] + '.json'), 'w') as json_file:
            json.dump({'label': labels[idx].tolist(),
                       'durations': durations[idx].tolist(),
                       'timestamps': array_to_list(segments_timestamps[idx].tolist())},
                      json_file)

    with open(os.path.join(config['output_folder'], 'config.json'), 'w') as json_file:
        json.dump({'classes': config['classes'],
                   'sampling_rate': config['sampling_rate'],
                   'length_segments_s': config['length_segments_s'],
                   'Training_files': array_to_list(segment_names[train_index]),
                   'Testing_files': array_to_list(segment_names[dev_test_index][dev_index]),
                   'Validation_files': array_to_list(segment_names[dev_test_index][test_index])},
                  json_file)


def main():
    r"""Generate audio segments and corresponding labels from Audioset files and CloudFactory labels.

        Audioset consists in 10 seconds long labelled video segments. The audio of these segments has been downloaded
        is re-labelled by CloudFactory with segmentation information to produce labelled 10 second long audio files.
        This script allows to generate, from these files, weakly labelled segments of smaller lengths by splitting
        the 10 seconds segments in smaller parts and getting the corresponding labels.
    """

    config = {"length_segments_s": 3.3,  # Length of smaller segments to generate

              "audioset_data_folder": '/home/vincent/Audio_blind_source_separation/Datadir/cloud_factorized_audioset/',
              "output_folder": "/home/vincent/Audio_blind_source_separation/Datadir"
                               "/audioset_segments_audio_and_labels",

              "sampling_rate": 16000,

              # Minimum portion of a segment that is labeled for the total segment to get the label
              "label_threshold": 0.07}

    # The argument in config are exposed as command line arguments. Parse these arguments.
    parser = argparse.ArgumentParser(allow_abbrev=False,
                                     description="Generate segments of given length from the 10 seconds audio "
                                                 "segments labelled by Cloud_factory. This program divides the "
                                                 "segments, which are saved as individual .wavs files. It also "
                                                 "computes the STFT magnitude, phase and mel spectrgram, the labels "
                                                 "associated with the segments and the durations of the labelled "
                                                 "portions in each segment. This data is saved in a hdf5 file. "
                                                 "Finally, that data is splitted (stratified split) into training, "
                                                 "development and test set, and each set is saved in a separate .hdf5 "
                                                 "file.")
    parser.add_argument("-l", "--length_segments_s", type=float, default=config['length_segments_s'],
                        help="Length (in seconds) of the segments to generate.")
    parser.add_argument('--audioset_data_folder', type=str, default=config['audioset_data_folder'],
                        help='Path to the folder containing the audioset audio and labels files.')
    parser.add_argument('--output_folder', type=str, default=config['output_folder'],
                        help='Path to the output folder for the generated segments. (Either non existing directory, '
                             'or empty directory)')
    parser.add_argument('--sampling_rate', type=int, default=config['sampling_rate'],
                        help='Sampling rate for the generated segments (the original audio files will be re-sampled '
                             'to this rate before use')

    parsed_args = vars(parser.parse_known_args()[0])
    config.update(parsed_args)

    # Get the list of all the audioset wav files.
    all_wavs_filenames = [os.path.join(dp, f)
                          for dp, dn, fn in os.walk(os.path.expanduser(config["audioset_data_folder"]))
                          for f in fn
                          if f.endswith('.wav')]

    # Get the list of all the label files produced by CloudFactory.
    all_labels_filenames = [os.path.splitext(wav_filename)[0] + '.txt' for wav_filename in all_wavs_filenames]

    # Get a list of the classes in the dataset. Add Human speech manually as it initially was not a class (added by
    # CloudFactory labels)
    classes = [directory for directory in os.listdir(config["audioset_data_folder"])
               if os.path.isdir(os.path.join(config["audioset_data_folder"], directory))] + ['Human Speech']
    config["classes"] = [a_class.replace(' ', '_').lower() for a_class in classes]

    # Create output folder if it does not exists. If it exists and is not empty: raise error.
    if not os.path.exists(config["output_folder"]):
        os.makedirs(os.path.join(config["output_folder"]))
    else:
        if os.listdir(config["output_folder"]):  # if folder is not empty
            raise ValueError('Output folder already exist !')

    # Generate the segments and associated information
    segment_names, labels, durations, segments_timestamps = generate_segments(all_wavs_filenames=all_wavs_filenames,
                                                                              all_labels_filenames=all_labels_filenames,
                                                                              config=config,
                                                                              classes=classes)

    # Merge the duplicated segments.
    segment_names, labels, durations, segments_timestamps = \
        merge_remove_duplicates(segment_names, labels, durations, segments_timestamps)

    # Perform training, testing and validation split, and save the labels and time stamps of each segments.
    split_files(segment_names, labels, durations, segments_timestamps, config)


if __name__ == '__main__':
    main()
